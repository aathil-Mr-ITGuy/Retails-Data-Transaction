
# Retail Data Pipeline Project

This project showcases an end-to-end data pipeline for retail transaction analysis. The pipeline includes data ingestion, processing, transformation, and visualization using various technologies.

## Project Overview

The project demonstrates the following processes:

1. **Data Ingestion**: CSV files are uploaded to Google Cloud Storage (GCS) using Apache Airflow.
2. **Data Processing**: The raw data is processed and transformed using Apache Airflow, dbt (data build tool), and Cosmos.
3. **Data Quality Checks**: Data quality checks are performed using Soda.
4. **Data Visualization**: Dashboards are created using Metaverse.

## Data Modeling
<img width="718" alt="Screenshot 2023-07-13 at 16 59 35" src="https://github.com/aathil-Mr-ITGuy/Retails-Data-Transaction/assets/36620321/36fb1506-df4d-4926-92a9-f3c432191a27">

## Pipeline
<img width="1509" alt="Screenshot 2023-07-13 at 16 41 19" src="https://github.com/aathil-Mr-ITGuy/Retails-Data-Transaction/assets/36620321/4dc31e47-14c2-40a2-93de-13d04cc6479d">

## Technologies Used

- **Apache Airflow**: For orchestrating the data pipeline.
- **Astro CLI**: For setting up the Airflow local environment.
- **dbt (data build tool)**: For transforming and modeling the data.
- **Soda**: For data quality checks.
- **Google Cloud Platform (GCP)**: For storing and processing data.
- **Metaverse**: For creating interactive dashboards.

## Dataset

The dataset used in this project can be found on Kaggle: - https://www.kaggle.com/datasets/tunguz/online-retail

## Prerequisites

Before running the data pipeline, make sure you have the following installed:

- Docker
- Astro CLI
- Soda
- Google Cloud Platform (GCP) account

## Setup

1. Clone this repository to your local machine.
2. Install Docker and the Astro CLI if you haven't already.
3. Set up your Google Cloud Platform (GCP) account and configure authentication.
4. Download the Online Retail dataset from Kaggle and place it in the appropriate directory.
5. Follow the instructions in the project documentation to configure and run the data pipeline.

## Usage

Follow the step-by-step instructions in the project documentation to set up and run the data pipeline.

## Contributing

Contributions are welcome! Feel free to submit pull requests or open issues for any improvements or suggestions.




## Author
- Aathil Ahamed
  - LinkedIn: [aathilks](https://www.linkedin.com/in/aathilks/)
  - Email: atldeae@gmail.com

